{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Red neuronal que simula el comportamiento de una compuerta lógica XOR\n",
    "\n",
    "Para esta primera red neuronal vamos a importar nada mas NumPy, creando la función de pérdida, los perceptrones y las capas desde cero con las herramientas que nos proporciona np. En este caso, vamos a crear una red de perceptrones multicapa con forward propagation, con 3 capas:\n",
    "* La primera es la de entrada (l0)\n",
    "* La segunda l1\n",
    "* La tercera l2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función de pérdida que usaremos será la función sigmoide:\n",
    "\n",
    "<img src = \"https://render.githubusercontent.com/render/math?math=S(x)%20=%20\\frac{\\mathrm{1}}{\\mathrm{1}%20%2B%20e%20^%20{-x}%20}\">\n",
    "\n",
    "O definida en su derivada para el descenso de gradiente:\n",
    "\n",
    "<img src = \"https://render.githubusercontent.com/render/math?math=S(x)%20=%20x%20*%20(1%20-%20x)\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def nonlin(x,deriv=False):\n",
    "\tif(deriv==True):\n",
    "\t    return x*(1-x)\n",
    "\n",
    "\treturn 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenemos este dataset de prueba para tres entradas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X = np.array([[0,0,1],\n",
    "            [0,1,1],\n",
    "            [1,0,1],\n",
    "            [1,1,1]])\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuestro array Y será el de valores reales que la red usará para ajustar su función de perdida:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Y = np.array([[0],\n",
    "\t      [1],\n",
    "\t      [1],\n",
    "\t      [0]])\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a inicializar el random con una seed para que se obtengan los mismos resultados siempre, syn0 y syn1 son las matrices de los pesos sinápticos de nuestra red.\n",
    "\n",
    "syn0 tiene 3 filas y 4 columnas, representando las 3 entradas que tiene y las 4 salidas, por su parte syn1 tiene 4 filas y 1 columna, representando sus 4 entradas y su única salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "syn0 = 2*np.random.random((3,4)) - 1\n",
    "syn1 = 2*np.random.random((4,1)) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí empieza la parte del entrenamiento con 60000 ciclos.\n",
    "\n",
    "Nuestra primera capa (l0) es los valores de entrada, la matriz X. La segunda capa (l1) es el resultado de aplicar el producto punto de la primera capa con los pesos sinápticos syn0 y luego pasar el resultado a través de nuestra función de pérdida, de manera análoga se hace lo mismo en la tercera capa (l2) con los resultados que obtuvimos de la primera capa y los pesos de la segunda.\n",
    "\n",
    "Después de realizar los cálculos, se tiene que estimar el error, haciendo la diferencia de los valores esperados (contenidos en la matriz Y) y los valores obtenidos de la tercera capa. Para poder hacer uso del descenso de gradiente para encontrar el mínimo local de la función, ocupamos obtener el producto punto de la primera derivada de nuestra función de pérdida y la diferencia de los valores. Posteriormente, también calculamos el error de la segunda capa, con el producto punto del error de la tercera capa y la transpuesta de la matriz de los pesos sinápticos de la segunda capa, todo esto usando la derivada de la función sigmoide para poder saber hacia donde ajustar los pesos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00260572]\n",
      " [0.99672209]\n",
      " [0.99701711]\n",
      " [0.00386759]]\n"
     ]
    }
   ],
   "source": [
    "for j in range(60000):\n",
    "\n",
    "    l0 = X\n",
    "    l1 = nonlin(np.dot(l0,syn0))\n",
    "    l2 = nonlin(np.dot(l1,syn1))\n",
    "    \n",
    "    l2_error = Y - l2\n",
    "    l2_delta = l2_error*nonlin(l2,deriv=True)\n",
    "\n",
    "    l1_error = l2_delta.dot(syn1.T)\n",
    "    \n",
    "\n",
    "    l1_delta = l1_error * nonlin(l1,deriv=True)\n",
    "\n",
    "    syn1 += l1.T.dot(l2_delta)\n",
    "    syn0 += l0.T.dot(l1_delta)\n",
    "    \n",
    "print(l2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
